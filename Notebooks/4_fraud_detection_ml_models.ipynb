{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83dc74e7",
   "metadata": {},
   "source": [
    "# Sub-task 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476b46f",
   "metadata": {},
   "source": [
    "To create a prototype system that can identify payments using machine learning we should adhere to the following procedure;\n",
    "\n",
    "## Step 1: Data Preprocessing\n",
    "\n",
    "Cleaning the Data; Address any information.\n",
    "\n",
    "Creating Informative Features; Extract characteristics from the available data.\n",
    "\n",
    "Transforming the Data;. Scale it as needed.\n",
    "\n",
    "Encoding Categorical Variables; Convert numerical columns (such as customer ID, gender and category) into a format suitable, for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2ed56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('bs140513_032310.csv')\n",
    "\n",
    "# Drop non-varying string columns\n",
    "data = data.drop(['zipcodeOri', 'zipMerchant'], axis=1)\n",
    "\n",
    "# Identify categorical columns for encoding\n",
    "categorical_cols = ['customer', 'age', 'gender', 'merchant', 'category']\n",
    "\n",
    "# Apply OneHotEncoder for categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply transformations\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99333dc8",
   "metadata": {},
   "source": [
    "To make categorical variables usable for machine learning algorithms we convert them into values using a technique called Label Encoding. This conversion is crucial for processing non numeric data.\n",
    "\n",
    "In order to determine whether a transaction is fraudulent the dataset is divided into two parts; features and the target variable.\n",
    "\n",
    "To evaluate the models performance on data it is customary, in machine learning to split the data into training and testing sets.\n",
    "\n",
    "By applying feature scaling we normalize the values of the features, which helps improve the performance of machine learning algorithms (Chawla et al. 2002).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414d0e7",
   "metadata": {},
   "source": [
    "## Step 2: Feature Scaling (Adjusted for Sparse Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "177b221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler with with_mean set to False\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "# Apply scaling to the transformed data\n",
    "X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "X_test_scaled = scaler.transform(X_test_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a9eb",
   "metadata": {},
   "source": [
    "## Step 3: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e7e5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=5000, solver='saga')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcb779",
   "metadata": {},
   "source": [
    "**Random Forest:** Chosen for its robustness in handling high-dimensional data and its ability to model complex, non-linear relationships. It's particularly effective in imbalanced datasets like those found in fraud detection scenarios (Bhattacharyya et al., 2011).\n",
    "\n",
    "**Gradient Boosting:** An ensemble learning method known for its high accuracy. Gradient Boosting constructs models in a stage-wise fashion and is well-suited for datasets where the relationships between features are complex (Dong et al., 2020).\n",
    "\n",
    "**Logistic Regression:** A simpler, linear model used as a baseline. Despite its simplicity, Logistic Regression can be very effective, especially when the underlying relationships in the data are not highly complex (Dal Pozzolo et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203c3a8",
   "metadata": {},
   "source": [
    "## Step 4:Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3b644",
   "metadata": {},
   "source": [
    "Each model is trained using the scaled training data. The training process involves adjusting the model parameters to best fit the data.\n",
    "\n",
    "Post-training, each model is evaluated on the test data. This step is crucial to assess the model's performance on unseen data, providing insights into its generalizability.\n",
    "\n",
    "The classification_report and confusion_matrix from scikit-learn are used for evaluation. These metrics are particularly important in the context of imbalanced datasets, such as those common in fraud detection. The focus is on metrics like precision, recall, and F1-score, which provide a more nuanced view of the model's performance, especially for the minority class (fraudulent transactions) (He & Garcia, 2009).\n",
    "\n",
    "High recall for the fraudulent class is critical in fraud detection to minimize false negatives. However, maintaining a balance with precision is necessary to avoid excessive false positives (Ngai et al., 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f32f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117512\n",
      "           1       0.90      0.81      0.86      1417\n",
      "\n",
      "    accuracy                           1.00    118929\n",
      "   macro avg       0.95      0.90      0.93    118929\n",
      "weighted avg       1.00      1.00      1.00    118929\n",
      "\n",
      "RandomForest Confusion Matrix:\n",
      "[[117391    121]\n",
      " [   268   1149]]\n",
      "------------------------------------------------------\n",
      "GradientBoosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117512\n",
      "           1       0.89      0.76      0.82      1417\n",
      "\n",
      "    accuracy                           1.00    118929\n",
      "   macro avg       0.95      0.88      0.91    118929\n",
      "weighted avg       1.00      1.00      1.00    118929\n",
      "\n",
      "GradientBoosting Confusion Matrix:\n",
      "[[117383    129]\n",
      " [   341   1076]]\n",
      "------------------------------------------------------\n",
      "LogisticRegression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117512\n",
      "           1       0.88      0.80      0.84      1417\n",
      "\n",
      "    accuracy                           1.00    118929\n",
      "   macro avg       0.94      0.90      0.92    118929\n",
      "weighted avg       1.00      1.00      1.00    118929\n",
      "\n",
      "LogisticRegression Confusion Matrix:\n",
      "[[117364    148]\n",
      " [   283   1134]]\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f'{name} Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'{name} Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fa99f",
   "metadata": {},
   "source": [
    "Based on the classification reports and confusion matrices for the RandomForest, GradientBoosting, and LogisticRegression models, let's analyze their performance in the context of fraud detection:\n",
    "\n",
    "**RandomForest Model**\n",
    "\n",
    "Precision: High precision for both classes, particularly for the fraudulent transactions (1) at 0.90.\n",
    "Recall: High recall for non-fraudulent transactions (0) and good recall for fraudulent transactions (1) at 0.81.\n",
    "F1-Score: Balanced F1-score for fraudulent transactions, reflecting a good balance between precision and recall.\n",
    "Confusion Matrix: 1149 out of 1417 fraudulent transactions correctly identified, with 268 false negatives.\n",
    "\n",
    "**GradientBoosting Model**\n",
    "\n",
    "Precision: Similar to RandomForest, with slightly lower precision for fraudulent transactions at 0.89.\n",
    "Recall: A bit lower recall for fraudulent transactions at 0.76, indicating more false negatives compared to RandomForest.\n",
    "F1-Score: Slightly lower than RandomForest, indicating a slight drop in the balance between precision and recall for fraudulent transactions.\n",
    "Confusion Matrix: 1076 out of 1417 fraudulent transactions correctly identified, with 341 false negatives.\n",
    "\n",
    "**LogisticRegression Model**\n",
    "\n",
    "Precision: Comparable to GradientBoosting for fraudulent transactions at 0.88.\n",
    "Recall: Similar to GradientBoosting for fraudulent transactions at 0.80.\n",
    "F1-Score: Slightly lower than RandomForest, indicating a balance between precision and recall.\n",
    "Confusion Matrix: 1134 out of 1417 fraudulent transactions correctly identified, with 283 false negatives.\n",
    "\n",
    "**Summary and Best Model Selection**\n",
    "\n",
    "RandomForest appears to be the best model among the three for this specific task. It has the highest recall for fraudulent transactions, which is crucial in fraud detection to minimize false negatives. Additionally, its precision is also high, indicating fewer false positives.\n",
    "GradientBoosting and LogisticRegression show similar performance, with GradientBoosting having a slightly lower recall for fraudulent transactions compared to RandomForest.\n",
    "\n",
    "**Overall Performance:** Considering the importance of correctly identifying fraudulent transactions (high recall) while maintaining a reasonable level of precision, RandomForest stands out as the most suitable model for this dataset and task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd64524",
   "metadata": {},
   "source": [
    "## Key Findings and Conclusion\n",
    "\n",
    "Based on the evaluation metrics obtained from analyzing the classification reports and confusion matrices of the RandomForest, GradientBoosting and LogisticRegression models we can draw the following findings and conclusions:\n",
    "\n",
    "**RandomForest Model**\n",
    "\n",
    "Precision for identifying transactions; 90%\n",
    "\n",
    "Recall, for identifying fraudulent transactions; 81%\n",
    "\n",
    "Overall Accuracy; 99.67% (117391 true non fraudulent + 1149 true fraudulent out of a total of 118929 transactions)\n",
    "\n",
    "**Conclusion:** The RandomForest model showcased the precision and recall in detecting transactions. With an 81% recall rate it successfully identified 81% of fraud cases minimizing instances where fraud goes undetected (Bhattacharyya et al., 2011). It also boasted a precision rate of 90% meaning that when it predicted a transaction as fraud it was highly likely to be accurate reducing alarms.\n",
    "\n",
    "**GradientBoosting Model**\n",
    "\n",
    "Precision for identifying transactions; 89%\n",
    "\n",
    "Recall for identifying transactions; 76%\n",
    "\n",
    "Overall Accuracy; 99.61% (117383 true non fraudulent +1076 true fraudulent out of a total of118929 transactions)\n",
    "\n",
    "**Conclusion:** The GradientBoosting model exhibited performance compared to RandomForest, particularly in terms of recall. Recall plays a role, in scenarios where failing to identify fraud can have consequences (Dong et al.,2020).\n",
    "\n",
    "**LogisticRegression Model**\n",
    "\n",
    "Precision for Fraudulent Transactions: 88%\n",
    "\n",
    "Recall for Fraudulent Transactions: 80%\n",
    "\n",
    "Overall Accuracy: 99.64% (117364 true non-fraudulent + 1134 true fraudulent out of 118929 total transactions)\n",
    "\n",
    "**Conclusion:** LogisticRegression performed comparably to GradientBoosting, indicating its effectiveness as a more straightforward model. However, it falls slightly behind RandomForest in terms of recall, which is a key metric in fraud detection (Dal Pozzolo et al., 2015).\n",
    "Based on the results the LogisticRegression model showed effectiveness to GradientBoosting. Slightly lagged behind RandomForest in terms of recall, which is an important metric in fraud detection (Dal Pozzolo et al., 2015).\n",
    "\n",
    "## Overall Conclusion \n",
    "In conclusion after analyzing models it was found that the RandomForest model performed best for this dataset. It demonstrated a balance between precision and recall when identifying transactions. This model effectively minimized negatives without increasing false positives making it highly suitable for fraud detection applications.\n",
    "\n",
    "However it is important to note that while RandomForest emerged as the performer in this analysis the selection of a model should always consider the specific requirements and characteristics of the dataset, at hand. In fraud detection scenarios maximizing recall (correctly identifying fraudulent transactions as possible) while maintaining a reasonable level of precision to avoid false positives is often prioritized (He & Garcia 2009; Ngai et al., 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5254f",
   "metadata": {},
   "source": [
    "## References\n",
    "1.  Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique.\n",
    "2.  Bhattacharyya, S., et al. (2011). Data mining for credit card fraud: A comparative study.\n",
    "3.  Dong, X., et al. (2020). A survey on ensemble learning.\n",
    "4.  Dal Pozzolo, A., et al. (2015). Calibrating Probability with Undersampling for Unbalanced Classification.\n",
    "5.  He, H., & Garcia, E.A. (2009). Learning from imbalanced data.\n",
    "6.  Ngai, E. W. T., et al. (2011). The application of data mining techniques in financial fraud detection:    A classification framework and an academic review of literature."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
